# Rag AI Agent using Azure OpenAI and Pinecone

This project consist of a simple RAG agent using Azure OpenAI and Pinecone for semantic search and context-aware generation.


# Architecture Diagram:


## Using Azure OpenAI Function Calling

Fetching documents using function calling in Azure OpenAI. AI decided when to call tool or not.

v0/ask -> Azure OpenAI(Decision Making - Function Calling?/General Query?) -> Generate Response 


## Using Semantic Search with Pinecone (RAG)
<img width="510" height="657" alt="Screenshot From 2026-01-11 11-34-53" src="https://github.com/user-attachments/assets/5471ccb3-5952-48b9-8ee2-b71836b5e5ca" />


### flow:
    - request receieved from the user
    - fetching previous conversation if any based on session id
    - retireving documents using semantic search
    - generating response based using Azure OpenAI model and context
    - updating session memory


# Tech Stack

## Tools
- Azure OpenAI models 
    - gpt-4o-mini for chat completion and embedding-3-small for vector embeddings
- Pinecone (Vector Database)
- FastAPI - web framework for exposing our services
- Pydantic - Data validation 
- python - programming language


## Deployments
- Azure App Service
- Containerization using Docker
- Docker public repository 



# Setup Instructions

## Local Setup 

1. fetch the code from github
```
git clone https://github.com/abhisheksingh/rag-ai-agent.git
```

2. create a virtual environment and install the requirements
```
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```
3. create .env and add credentials
```
touch .env

AZURE_OPENAI_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXX
AZURE_OPENAI_INFERENCE_ENDPOINT=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_DEPLOYMENT_NAME=xxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_API_VERSION=xxxxxxxxxxxxxxxxxxxxxxxxx
PINECONE_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_ENDPOINT=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```
4. run the app
```
uvicorn main:app --reload
```

## Cloud Setup (Azure Deployment using App service and Docker)

### pre-requisites
- need to have docker account
- need to have azure account

1. create a docker image
```
docker login
docker build -t rag-ai-agent .
```

2. go to https://hub.docker.com/repository/ and create a repository e.g. rag-ai-agent

3. tag the image
```
docker tag rag-ai-agent:latest rag-ai-agent:latest
```

4. push the image to docker repository
```
docker push rag-ai-agent:latest
```

5. create azure app service -> select container app -> no database -> connect to your public docker registrty -> keep rest at deault -> create app service

<img width="906" height="763" alt="Screenshot From 2026-01-11 12-36-01" src="https://github.com/user-attachments/assets/d903ca1a-82fa-4790-8ebe-134b6bd46f96" />
<img width="906" height="763" alt="Screenshot From 2026-01-11 12-32-36" src="https://github.com/user-attachments/assets/f1c3ac67-132f-4702-a59a-851a2ff2d061" />



# Run the RAG ingestion Script

The script runs a simple ingestion pipeline - extract document text, chunk it and upsert it to the vector database.
```
python -m venv venv && source venv/bin/activate
pip install -r requirements.txt 
python rag_ingestion.py
```
flow : 

# APIs

## 1. /ask endpoint - It will provide response generated by RAG AI Agent
Run the following CURL command to test the /ask endpoint:
```
curl -X POST "https://domain/v0/ask" -H "Content-Type":"application/json" -d '{"question":"who is acmecloud?","session_id":"user123"}'
```
## 2. /v0/ask endpoint - It will provide response generated by Simple Azure OpenAI that using Function Calling to fetch documents
Run the following CURL command to test the /v0/ask endpoint:
```
curl -X POST "https://domain/v0/ask" -H "Content-Type":"application/json" -d '{"question":"who is acmecloud?","session_id":"user123"}'    
```



# Quickly view logs via Log streaming in Real-time.
<img width="1913" height="167" alt="Screenshot From 2026-01-11 14-37-11" src="https://github.com/user-attachments/assets/9c95680c-6ef4-497e-9263-284c1b6ad4b3" />


# Design Approach
## 1. Separation of Ingestion and Query Pipelines
The document ingestion workflow (PDF loading, text cleaning, chunking, and vector upsert) is intentionally separated from the runtime query workflow.
Ingestion is implemented as a standalone, re-runnable script, while FastAPI handles only real-time user queries. 

## 2. Use of Pinecone for Vector Search
Pinecone was chosen as the vector database due to its managed, scalable, and low-latency semantic search capabilities.

## 3. Sentence-Preserving Chunking Strategy
Documents are chunked using sentence-based tokenization rather than fixed token slicing. This preserves semantic coherence within each chunk, improving retrieval quality and reducing hallucinations during generation. If not used, the chunks might be too long and might contain confusing whitespacing or line breaking that can lead to hallucinations.

## 4. Session-Based Memory Handling
Conversation history is maintained using a session-based memory abstraction. This allows contextual follow-up questions without storing long-term chat history in the vector database, reducing cost and complexity.


## 5. Function Calling to Fetch Relevant Documents
used to allow the model to decide whether document retrieval is required or if a general response is sufficient. This optimizes token usage and avoids unnecessary vector searches for simple queries.


# Limitations 
- manually igestion of documents
- memory is not persistent
- uses semantic search only 
- No Incremental or Duplicate Detection to check if the same document is ingested multiple times
- documents are copied into docker leading to relative image size increase.
- Supports PDF documents only

# Future Improvements
- Add Incremental or Duplicate Detection 
- Add memory persistence
- Improve error handling and logging
- Support other document formats


If you have any questions or feedback, please don't hesitate to reach out.
